---
marp : true
auto-scaling:
    - true
    - fittingHeader
    - math
    - code
paginate : true
theme : hegel
title : Optimizaci√≥n mediante algoritmos por refuerzo
author :
   - Alberto D√≠az √Ålvarez <alberto.diaz@upm.es>
   - Raul Lara Cabrera <raul.lara@upm.es>
description : Optimizaci√≥n. Curso 2022-2023. E.T.S.I. Sistemas Inform√°ticos (UPM)
math: katex
---

<!-- _class: titlepage -->
![bg left:33% width:100%](https://upload.wikimedia.org/wikipedia/commons/1/1b/Reinforcement_learning_diagram.svg)

<div class="title">Optimizaci√≥n mediante algoritmos por refuerzo</div>
<div class="subtitle">Rob√≥tica</div>
<div class="author">Alberto D√≠az y Ra√∫l Lara</div>
<div class="date">Curso 2022/2023</div>
<div class="organization">Departamento de Sistemas Inform√°ticos</div>

[![height:30](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-informational.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)

---

# Paradigmas de aprendizaje en <i>Machine Learning</i>

**Supervisado**: Se aprende de ejemplos con sus correspondientes respuestas.

- Problemas de regresi√≥n y clasificaci√≥n.

**No supervisado**: B√∫squeda de patrones en datos no etiquetados.

- Problemas de <i>clustering</i>, reducci√≥n de la dimensionalidad, recodificaci√≥n, ...

**Por refuerzo**: Se aprende a trav√©s de la experiencia a base de recompensas.

- Problemas de aprendizaje de pol√≠ticas de decisi√≥n.

---

<!-- _class: cite -->
 
<div class="cite-author" data-text="Edward Thorndike - Law of Effect">

   "De varias respuestas dadas al mismo hecho, las seguidas de satisfacci√≥n para el animal estar√°n, en igualdad de condiciones, m√°s firmemente conectadas con este, de modo que tender√°n a repetirse; las seguidas de incomodidad para el animal tendr√°n, en igualdad de condiciones, sus conexiones con el hecho debilitadas, de modo que tender√°n a ocurrir menos. Cuanto mayor sea la satisfacci√≥n o el malestar, mayor ser√° el refuerzo o el deterioro del v√≠nculo."

</div>

---

# Aprendizaje por refuerzo (RL)

√Årea del <i>machine learning</i> donde **los agentes aprenden interactuando**:

- **Imita** de manera fundamental el **aprendizaje** de muchos **seres vivos**.
- Esa interacci√≥n produce tanto resultados deseados como no deseados.
- Se entrena con la **recompensa o castigo** determinados para dicho resultado.
- El agente tratar√° de maximizar la recompensa a largo plazo.

Se utiliza principalmente en dos √°reas hoy en d√≠a:

- **Juegos**: Los agentes aprenden las reglas y las jugadas jugando.
- **Control**: Los agentes aprenden en entornos de simulaci√≥n las mejores pol√≠ticas de control para un problema determinado.

> Un ejemplo curioso es el publicado en <https://www.nature.com/articles/nature14236>, donde describen c√≥mo un agente aprende a jugar a 49 juegos de Atari 2600 llegando a un nivel de destreza comparable al humano.

---

# Terminolog√≠a

**Agente inteligente**: Entidad que interact√∫a con el **entorno**.

- Usaremos indistintamente los conceptos de agente, agente inteligente y robot.
- **Entorno**: El mundo f√≠sico o virtual con el que interactua el agente.

**Estado** y **observaci√≥n**: Informaci√≥n que el agente obtiene del entorno:

- Estado $S_i$: Descripci√≥n **completa** del estado del entorno (e.g. juego del Go).
- Observaci√≥n $O_i$: Descripci√≥n **parcial** del estado del entorno (e.g. Warcraft II).

**Espacio de acciones**: Conjunto de acciones que puede realizar el agente:

- **Discreto**: El conjunto es finito (e.g. juego del Go).
- **Continuo**: El conjunto es infinito (e.g. veh√≠culo aut√≥nomo).

---

# Ejemplo #1: Juego del Go

![bg left:33%](https://upload.wikimedia.org/wikipedia/commons/d/de/Go_captura_01.png)

- Agente: Robot que juega al Go.
- Entorno/mundo: El tablero en el que se juega.
- Estado: Colocacion concreta de las piedras.
- Observaci√≥n: Estado (sin informaci√≥n oculta).
- Espacio de acciones (finito): Poner piedra en una casilla vac√≠a.

---

# Ejemplo #2: Warcraft II

![bg left:33%](https://www.gamespot.com/a/uploads/original/gamespot/images/2006/features/greatestgames/warcraft2/712467-warcraft2_001.jpg)

- Agente: Robot que juega al Warcraft II.
- Entorno/mundo: Pantalla en la que se juega.
- Estado: Situaci√≥n de la pantalla en un momento determinado.
- Observaci√≥n: Lo que el agente ve en un instante determinado (sin la niebla de guerra).
- Espacio de acciones (finito): Mover unidades, construir edificios, ...

---

# Ejemplo #3: Coche aut√≥nomo

![bg left:33%](https://insia-upm.es/wp-content/uploads/2021/12/coche_autonomo5_800-1280x768-1-980x588.webp)

- Agente: Robot que conduce el veh√≠culo.
- Entorno/mundo: El continente en el que se encuentra el veh√≠culo.
- Estado: Estado del continente en un momento determinado.
- Observaci√≥n: Lo que el agente ve por sus sensores en un instante determinado.
- Espacio de acciones (infinito): Girar el volante un determinado √°ngulo, aumentar y disminuir aceleraci√≥n, ...

---

# Modelo de interacci√≥n agente-entorno

El proceso de aprendizaje por refuerzo es el siguiente:

<div class="columns">
<div class="column">
<center>

![Diagrama del modelo de interacci√≥n agente-entorno](../img/t4/Modelo%20agente-entorno.svg)

</center>
</div>
<div class="column">

1. El agente lee un estado $S_0$ del entorno.
2. De acuerdo a $S_0$, realiza la acci√≥n $A_0$.
3. El entorno pasa al nuevo estado $S_1$.
4. El agente recibe una recompensa $R_1$.

</div>
</div>
<hr>

Este bucle produce una secuencia de estados, acciones y recompensas:

$$S_0, A_0, R_1, S_1, A_1, \ldots$$

---

# Hip√≥tesis de la recompensa

El agente quiere **maximizar la recompensa acumulada** (rendimiento esperado).

- La recompensa es el <i>feedback</i> que el entorno da al agente.
- Le permite saber al agente si la acci√≥n es buena o no.

La recompensa acumulada es la suma de todas las recompensas de la secuencia.

$$ R(\tau) = \sum_{i=0}^\infty \gamma^i R_{t+i+1} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots$$

Sin embargo, las recompensas no tienen por qu√© tener todo su valor siempre.

- De ah√≠ el factor de ajuste en la recompensa, o $\gamma \in [0, 1]$.

---

# Ajuste en la recompensa

Supongamos el siguiente ejemplo no tan raro de la realidad que nos espera:

<div class="columns">
<div class="column" style="margin: 0 auto;">
<pre>
üßü‚¨úüöóüßüüèπ‚¨úüßü
‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú
‚¨úü•ò‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú
‚¨ú‚¨ú‚¨ú‚¨úüç∏‚¨ú‚¨ú
‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú
‚¨úüçî‚¨ú‚¨ú‚¨úüçé‚¨ú
‚¨ú‚¨ú‚¨úüë©‚Äçü¶∞‚¨ú‚¨ú‚¨ú
</pre>
</div>
<div class="column">

- El agente se mueve una celda cada $t$.
- Los zombies tambi√©n.
- Objetivo: Conseguir el mayor n√∫mero de puntos de supervivencia...
- Si nos ara√±an perdemos.

</div>
</div>

$\gamma$ indica si interesan m√°s recompensas a **corto** ($\gamma \approx 0$)) o a **largo** ($\gamma \approx 0$) **plazo**.

Las recompensas que llegan antes tienen m√°s probabilidades de suceder.

   Esto es porque son m√°s predecibles que las recompensas a largo plazo

---

# Tareas y problemas

Se entiende por tarea a una instancia de un problema. Existen dos tipos diferenciados:

Tenemos dos tipos bien diferenciados de tareas:

- **Epis√≥dicas**: Poseen estado inicial y terminal o final (e.g. Sonic the Hedgehog).
- **Continuas**: Tarea que no posee estado terminal (e.g. veh√≥culo aut√≥nomo).

---

Por cierto, aprovechamos para definir:

- **Probabilidad de transici√≥n**: $P(S_{t+1} | (S_t, A_t), (S_{t-1}, A_{t-1}), \ldots, (S_1, A_1))$
- **Probabilidad de recompensa**: $P(R_{t+1} | (S_t, A_t), (S_{t-1}, A_{t-1}), \ldots, (S_1, A_1))$

---

# Resoluci√≥n de problemas

Pol√≠tica $\pi$ (de <i>policy</i>): Funci√≥n que asigna una acci√≥n $A$ a un estado determinado $S$.

$$\pi(S) = A$$

Objetivo: Encontrar la funci√≥n $\pi$ que **maximiza el rendimiento esperado cuando el agente act√∫a de acuerdo con ella.**

Existen dos m√©todos para aprender esta funci√≥n $\pi$:

- **Directo**: Qu√© acci√≥n debe realizar en el estado actual.
- **Indirecto**: Qu√© estados son mejores para tomar la acci√≥n que lleva a esos estados.

---

# M√©todos directos (basados en pol√≠ticas)

En estos m√©todos **aprendemos directamente la funci√≥n $\pi$**. Existen dos tipos:

<div class="columns">
<div class="column">

## Determinista

Devuelve **siempre la misma acci√≥n** para un estado determinado.

$$\pi(S) = A$$

<hr>

Por ejemplo:

$$\pi(S_i) = \{‚ñ∫\}$$

</div>
<div class="column">

## No determinista

Devuelve una **distribuci√≥n de probabilidad** sobre las acciones.

$$\pi(S) = P[A | S]$$
<hr>

Por ejemplo:

$$\pi(S_i) = \{(‚óÑ, 0.3), (‚ñ∫, 0.5), (‚ñº, 0.1), (‚ñ≤, 0.1)\}$$

</div>
</div>

---

# M√©todos indirectos (basados en valores)

Aprendemos una funci√≥n $V$ que **relaciona un estado con su valor estimado**.

- Valor: Recompensa acumulada si empieza en ese estado y se mueve al mejor estado.
- El agente selecciona la acci√≥n de mayor valor


<div class="columns">
<div class="column">

## Valor estado

$$V_\pi(S) = \mathbb{E}_\pi[R_t | S_t = S]$$

</div>
<div class="column">

## Valor par estado-acci√≥n

$$V_\pi(S, A) = \mathbb{E}_\pi[R_t | S_t = S, A_t = A]$$

</div>
</div>

<hr>

Independientemente de la funci√≥n elegida, el resultado ser√° la recompensa esperada.

- Ojo: **Para** calcular **cada valor de un estado** (o par estado-acci√≥n), hay que **sumar todas las recompensas** que puede obtener un agente si empieza en ese estado.

---

# Ecuaci√≥n de Bellman

**Simplifica el c√°lculo del valor** del estado o del par estado-acci√≥n.

Con lo que sabemos hasta ahora, sabemos que si calculamos $V(S_t)$ tenemos que calcular la recompensa a partir de ese estado y luego seguir la pol√≠tica para siempre.

Por tanto, para calcular $V(S_t)$, necesitamos hacer la suma de las recompensas esperadas:

---

# Comparativa entre m√©todos directos e indirectos

<div class="columns">
<div class="column">

## M√©todos directos

<img src="../img/t4/policy-based-method.png" style="width: 95%;">

La **pol√≠tica √≥ptima** se encuentra **entrenando** la pol√≠tica **directamente**.

</div>
<div class="column">

## M√©todos indirectos

<img src="../img/t4/value-based-method.png" style="width: 95%;">

Encontrar una **funci√≥n de valor √≥ptima** lleva a tener una **pol√≠tica √≥ptima**.

</div>
</div>

<hr>

Por lo tanto Independientemente del m√©todo, tendremos una pol√≠tica.

- Pero en el caso de los m√©todos basados en valores no la entrenamos.
- Ser√° una "simple" funci√≥n que usar√° los valores dados por la funci√≥n $V$.

---

# Estrategia epsilon-greedy

Pol√≠tica sencilla para elegir acci√≥n que mantiene el equilibrio exploraci√≥n/explotaci√≥n.

- El agente elige una acci√≥n de forma aleatoria con probabilidad $\epsilon$
- La mejor acci√≥n conocida con probabilidad $1-\epsilon$.

Por lo general, se empieza con un √©psilon alto (mucha exploraci√≥n).

- Seg√∫n el agente aprende m√°s sobre el entorno, epsilon disminuye.
- Ha explorado mucho, as√≠ que puede centrarse en explotar lo conocido.

---

# <i>Q-Learning</i>

Es un m√©todo indirecto (m√©todo basado en valores)


---


---

# Ejemplo: Hambre y zombies

Objetivo: Utilizar t√©cnicas de RL para que el superviviente llegue a su destino.

<hr>
<div style="margin: 0 auto;">
<pre>
üè¢üóªüè¢üè¢üè¢üè¢üè¢üè¢üè¢üè¢
üè¢‚¨õ‚¨õ‚¨õüßüüè¢‚¨õ‚¨õ‚¨õüßü
üè¢‚¨õüè¢‚¨õüè¢üè¢‚¨õüè¢üè¢üè¢
üè¢‚¨õüè¢ü•òüè¢ü•ò‚¨õüçîüçîüçî
üè¢‚¨õüè¢üè¢üè¢üè¢‚¨õüè¢üè¢üè¢
üè¢‚¨õüçîüè¢‚¨õ‚¨õ‚¨õ‚¨õ‚¨õüè¢
üè¢‚¨õ‚¨õ‚¨õ‚¨õüè¢üßüüè¢‚¨õüè¢
üè¢üè¢üè¢üè¢üè¢üè¢üè¢üè¢ü§∞üè¢
</pre>
</div>
<hr>

Hay que comenzar considerando los estados, las acciones y las recompensas.

---

# Estados

El agente se encuentra en un estado y toma una acci√≥n de acuerdo a este.

Espacio de estados: Todas las situaciones posibles en las que se puede encontrar el agente.

- Debe contener informaci√≥n suficiente para tomar una decisi√≥n correcta.

En el ejemplo, son todas las posiciones que podr√≠a ocupar el agente (35).

- Podr√≠amos complicarlo m√°s, por ejemplo, obligando a llevar comida.
- Esto implicar√≠a los 35 estados con y sin comida encima (35 + 35 = 70).
- Pero nos quedaremos con el ejemplo simple.

---

# Acciones

El agente se encuentra con uno de los 35 estados y realiza una acci√≥n.

- 5 acciones posibles: arriba, abajo, izquierda, derecha y coger comida.

**Espacio de acciones**: Conjunto de todas las acciones posibles para un estado.

---

# Recompensas

El superviviente est√° motivado por la recompensa, as√≠ que aprender√° a:

- Encontrar la comida y el objetivo.
- Evitar las zonas infestadas de zombies.

Algunos puntos a tener en cuenta para el agente:

- Alta recompensa por llegar a las monta√±as üóª (+1000); es el objetivo.
- Ligera recompensa por encontrar comida ü•òüçî (+10) porque est√° bien.
- Penalizaci√≥n si llega a un zombie üßü (-50) porque no interesa en absoluto.

Es importante tener en cuenta que la recompensa no siempre es inmediata:

- Puede haber tramos sin nada hasta llegar a un estado muy bueno.

---

# <i>OpenAI Gym</i><!--_class: transition-->

---

[CREAR EL ENTORNO DE OPENAI GYM PARA EL EJEMPLO: <https://towardsdatascience.com/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e>]

---

# El entorno <i>OpenAI Gym</i>

OpenAI Gym es una biblioteca de entornos de aprendizaje por refuerzo.

- Proporciona entornos de juego para probar nuestros agentes desarrollados.

Se encarga de proporcionar toda la informaci√≥n que el agente necesitar√≠a:

- Entorno, posibles acciones y recompensas, estado actual, ...
- S√≥lo tenemos que preocuparnos de la l√≥gica del agente.

El ejemplo anterior se ha implementado como entorno para practicar con √©l.

---

# Instalaci√≥n

La biblioteca est√° disponible a trav√©s de Pypi:

```bash
pip install gym
```

Una vez instalada, podemos cargar el entorno del juego y mostrar su aspecto:

```python
import gym

env = gym.make("Starvation and Zombies").env
env.render()
```

---

# Propiedad de M√°rkov

El estado futuro del proceso depende del estado actual, y no de los anteriores.

- Es un estado que cumplen ciertos procesos estoc√°sticos.
- Definida por Andr√©i Markov en 1906 en su Teor√≠a de Cadenas de M√°rkov.

Al proceso que satisface esta propiedad se denomina **Proceso de M√°rkov**.

- Concretamente se denominan Procesos de M√°rkov de **primer orden**.
- La definici√≥n se puede extender a $n$ estados anteriores (proceso de orden $n$).
- Si el espacio de estados es finito, equivale a una **cadena de M√°rkov**.

Si hay que quedarse con algo, nos dice que nuestro agente s√≥lo necesita el estado actual para decidir qu√© acci√≥n tomar.

---

# Procesos de decisi√≥n de Markov (MDP) en agentes

Se asume que el proceso de decisi√≥n de un agente es un MDP:

- $P(s_{t+1} | (s_t, a_t), (s_{t-1}, a_{t-1}), \ldots, (s_1, a_1)) = P(s_{t+1} | (s_t, a_t))$
- $P(s_{r+1} | (s_t, a_t), (s_{t-1}, a_{t-1}), \ldots, (s_1, a_1)) = P(r_{t+1} | (s_t, a_t))$

Los procesos de decisi√≥n de un 

Sistemas de toma de decisiones basados en procesos de M√°rkov. Incluyen:

- $S$: Conjunto finito de estados.
- $A$: Conjunto finito de acciones.
- $P(s_i|(s_j, a)$: Probabilidad de transici√≥n de $s_i$ a $s_j$ con la acci√≥n $a$.
- $\pi : S \rightarrow A$: Funci√≥n que define las pol√≠ticas de decisi√≥n.
- 
- **Transiciones** entre estados.
- **Recompensas** por transici√≥n. Pueden ser positivas o negativas.
- Factor de descuento $\gamma \in [0, 1]$: Importancia entre recompensas inmediatas o futuras (generalmente $\gamma^t$). <!-- Si por ejemplo gamma es 0.9 y hay una recompensa de 100 a 5 pasos del punto en el que estamos, la recompensa realmente ser√° de 100 * 0.9^5 -->
- Memoria: En orden 1 no es necesaria memoria pero en √≥rdenes mayores s√≠.

---

# MDP en nuestro ejemplo

El objetivo del superviviente es intentar maximizar la suma de las recompensas futuras tomando la mejor acci√≥n para cada estado:

$$\sum_{t=0}^\infty r_{e_t, a_t} \cdot \gamma^t$$

Explicado:

1. Estamos sumando para cada paso de tiempo $t$, de ah√≠ el sumatorio.
2. Cada paso de tiempo tiene una recompensa $r_{e_t, a_t}$ asociada la acci√≥n tomada.
3. $\gamma^t$ es el factor de descuento en 1 por ahora y olvid√©monos de ello.

Una vez formalizado el problema, vamos a explorar algunas soluciones.

---

# Soluci√≥n #1: Q-learning

Se apoya en una funci√≥n denominada  acci√≥n-valor (<i>action-value</i>) o funci√≥n $Q$:

- Entrada: Estado y acci√≥n a realizar.
- Salida: Recompensa esperada de esa acci√≥n (y de todas las posteriores).

La funci√≥n $Q$ se actualiza de forma iterativa:

1. Antes de explorar el entorno, $Q$ da el mismo valor fijo (arbitrario).
2. Seg√∫n se explora, aproxima mejor el valor de la acci√≥n $a$ en un estado $s$.
3. Seg√∫n se avanza, la funci√≥n $Q$ se actualiza.

Representa suma de las recompensas de elegir la acci√≥n $Q$ y todas las acciones √≥ptimas posteriores.

---

$$Q(e_t, a_t) = Q(e_t, a_t) + \alpha \cdot (r_t + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t))$$

Realizar $a_t$ en el estado $e_t$ actualiza su valor con un t√©rmino que contiene:

- $\alpha$: Lo "agresivo" que sstamos haciendo el entrenamiento.
- $r_t$: Estimaci√≥n que obtuvimos al actuar en el estado $e_t$ anteriormente.
- $\max_a Q(s_{t+1}, a)$: Recompensa futura estimada.
- Se resta adem√°s el valor antig√ºo para incrementar o disminuir la diferencia en la estimaci√≥n.

Ahora tenemos una estimaci√≥n de valor para cada par estado-acci√≥n.

- Con el podemos elegir la acci√≥n que nos interesa (e.g. usando epsilon-greedy)

---

# Soluci√≥n #2: <i>Policy learning</i>

Trata de determinar una funci√≥n $\pi$ asigna la mejor acci√≥n a un estado dado:

$$a = \pi(e)$$

<cite>"Cuando observo el estado $e$, lo mejor que puedo hacer es tomar la acci√≥n $a$"</cite>

Esta funci√≥n es una funci√≥n compleja que tratamos de aproximar.

- Y lo m√°s "sencillo" y r√°pido es usar redes neuronales para ello.

---

# Otras soluciones

## <i>Deep $Q$-networks</i> (DQN)

Son aproximaciones de funciones $Q$ utilizando redes neuronales profundas<sup>2</sup>.

## Asynchronous Advantage Actor-Critic (A3C)

Es una combinaci√≥n de las dos t√©cnicas anteriores<sup>3</sup>, combinando:

- Un actor: Red de pol√≠ticas de actuaci√≥n que deciden qu√© acci√≥n tomar.
- Un cr√≠tico: DQN que decide el valor de cada acci√≥n a tomar.

> <sup>2</sup> <https://www.nature.com/articles/nature14236>
> <sup>3</sup> <https://proceedings.mlr.press/v48/mniha16.html>

---

<video controls width=100% src="https://drive.upm.es/s/0VIKqV7AiEQSzPu/download"></video>

---

# Relevancia del aprendizaje por refuerzo<!--_class: transition-->

---

<!-- _class: cite -->
 
<div class="cite-author">

   "El Go es un juego estudiado por los humanos durante m√°s de 2500 a√±os. AlphaZero, en un tiempo insignificante (3 d√≠as), pas√≥ de conocer s√≥lo las reglas del juego a vencer a los mejores jugadores del mundo, superando todo nuestro conocimiento acumulado durante milenios. Ning√∫n campo del aprendizaje autom√°tico ha permitido avanzar tanto en este tipo de problemas como el aprendizaje por refuerzo."

</div>

---

# Relevancia del aprendizaje por refuerzo hoy en d√≠a

Podemos decir que es pr√°cticamente el √∫nico paradigma de aprendizaje:

- Capaz de aprender comportamientos complejos en entornos complejos.
- Que ha podido hacerlo pr√°cticamente sin supervisi√≥n humana.

Ofrece a la rob√≥tica forma abordar c√≥mo dise√±ar comportamientos dif√≠ciles.

- Que por otro lado, son pr√°cticamente todos.
- Las cosas f√°ciles para un humano suelen ser las m√°s complejas de dise√±ar.

Permite a robots descubrir de forma aut√≥noma comportamientos √≥ptimos:

- No se detalla la soluci√≥n al problema, sino que se interacciona con el entorno.
- La retroalimentaci√≥n de el efecto sobre el entorno permite aprender.

---

# La utilidad de los modelos aproximados

Los datos del mundo real pueden usarse para aprender modelos aproximados.

- Mejor, porque el proceso de aprendizaje por ensayo y error es muy lento.
- Sobre todo en un sistema que tiene que hacerlo en un entorno f√≠sico.
- Las simulaciones suelen ser mucho m√°s r√°pidas que el tiempo real.
- Y tambi√©n tambi√©n mucho m√°s seguras para el robot y el entorno
- <i>**Mental rehearsal**</i>: Describe el proceso de aprendizaje en simulaci√≥n.

Suele ocurrir que un modelo aprende en simulaci√≥n pero falla en la realidad:

- Esto se conoce como **sesgo de simulaci√≥n**.
- Es an√°logo al sobreajuste en el aprendizaje supervisado.
- Se ha demostrado que puede abordarse introduciendo modelos estoc√°sticos. <!-- Incluso si el sistema es muy cercano al determinismo. -->

---

# Impacto del uso de conocimiento o informaci√≥n previa

El conocimiento previo puede ayudar a guiar el proceso de aprendizaje:

- Este enfoque reduce significativamente el espacio de b√∫squeda.
- Esto produce una **aceleraci√≥n** dram√°tica **en el proceso de aprendizaje**.
- Tambi√©n **reduce la posibilidad de encontrar mejores √≥ptimos**<sup>1</sup>.

Existen dos t√©cnicas principales para introducir conocimiento previo:

- A trav√©s de la **demostraci√≥n**: Se da una pol√≠tica inicial semi-exitosa.
- A trav√©s de la **estructuraci√≥n de la tarea**: Se da la tarea dividida.

> <sup>1</sup> Alpha Go fue entrenado con un conocimiento previo de Go, pero Alpha Go Zero no sab√≠a nada del juego. El resultado fue que Alpha Go Zero jug√≥ y gan√≥ a Alpha Go en 100 partidas.
---

# Desaf√≠os del aprendizaje por refuerzo

**La maldici√≥n de la dimensionalidad**: El espacio de b√∫squeda crece exponencialmente con el n√∫mero de estados.

**La maldici√≥n del mundo real**: El mundo real es muy complejo y no se puede simular.

- Desgaste, estocasticidad, cambios de din√°mica, intensidad de la luz, ...

**La maldici√≥n de la incertidumbre del modelo**: El modelo no es perfecto y no se puede simular.

- Cada peque√±o error se acumula, haciendo que conseguir un modelo suficientemente preciso del robot y su entorno sea un reto

---

# Algunas reflexiones

¬øNosotros como humanos tenemos funciones de valor? ¬øC√≥mo definimos la recompensa que maximizamos en nuestra vida real?

- M√°s all√° del placer y el dolor, tiende a incluir otros elementos complicados como el bien y el mal, la satisfacci√≥n, el amor, la espiritualidad, etc.

La pregunta central de la filosof√≠a moral es: ¬øqu√© debemos hacer?

- ¬øC√≥mo debemos vivir? ¬øQu√© acciones son correctas o incorrectas?
- Una posible respuesta es que, claramente, depende de los valores de cada uno.

A medida que vayamos creando una IA cada vez m√°s avanzada, √©sta empezar√° a salir de los problemas donde la recompensa se define mediante un n√∫mero de puntos ganados en el juego, y requerir√° recompensas m√°s complejas.

- Los veh√≠culos aut√≥nomos, por ejemplo, son agentes que tienen que tomar decisiones con una definici√≥n de recompensa algo m√°s compleja
- Al principio, la recompensa podr√≠a estar ligada a algo como "llegar a salvo al destino".
- Pero ¬øy si se ve obligado a elegir entre mantener el rumbo y atropellar a cinco peatones o desviarse y atropellar a uno? ¬ødebe desviarse o incluso da√±ar al conductor con una maniobra peligrosa? ¬øY si el √∫nico peat√≥n es un ni√±o, o un anciano, o el pr√≥ximo Einstein o Hitler? ¬øCambia eso la decisi√≥n? ¬øpor qu√©? ¬øY si al dar un volantazo tambi√©n se destruimos una escultura extremadamente valiosa e irremplazable?
- De repente tenemos un problema mucho m√°s complejo cuando intentamos definir la funci√≥n objetivo, y las respuestas no son tan sencillas.

Nosotros, como humanos, ¬øc√≥mo sabemos lo que es correcto o no? Generalmente podemos responder "por intuici√≥n".

- Pero ponerlo en palabras o reglas es sencillamente imposible.
- Sin embargo, quiz√°s sea posible que una m√°quina aprenda estos valores, esta "intuici√≥n", de alguna manera.
- Este puede ser uno de los problemas t√©cnicos m√°s importantes que los humanos tendr√°n que resolver en un futuro.

---

# ¬°GRACIAS!<!--_class: transition-->
